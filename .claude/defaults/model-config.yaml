# Hounfour Default Configuration (SDD §4.1.2)
#
# System Zone defaults — ships with the Loa framework.
# Users override in .loa.config.yaml under hounfour: section.
# DO NOT EDIT — use project config for customization.

# Provider registry
providers:
  openai:
    type: openai
    endpoint: "https://api.openai.com/v1"
    auth: "{env:OPENAI_API_KEY}"
    models:
      gpt-5.2:
        capabilities: [chat, tools, function_calling]
        context_window: 128000
        token_param: max_completion_tokens
        pricing:
          input_per_mtok: 10000    # $10.00 per million tokens (micro-USD)
          output_per_mtok: 30000   # $30.00 per million tokens
      gpt-5.3-codex:
        capabilities: [chat, tools, function_calling, code]
        context_window: 400000
        token_param: max_completion_tokens
        pricing:
          input_per_mtok: 1750     # $1.75 per million tokens
          output_per_mtok: 14000   # $14.00 per million tokens

  google:
    type: google
    endpoint: "https://generativelanguage.googleapis.com/v1beta"
    auth: "{env:GOOGLE_API_KEY}"
    models:
      gemini-2.5-flash:
        capabilities: [chat]
        context_window: 1048576
        pricing:
          input_per_mtok: 150000     # $0.15 per million tokens (micro-USD)
          output_per_mtok: 600000    # $0.60 per million tokens
      gemini-2.5-pro:
        capabilities: [chat, thinking_traces]
        context_window: 1048576
        pricing:
          input_per_mtok: 1250000    # $1.25 per million tokens
          output_per_mtok: 10000000  # $10.00 per million tokens
        extra:
          thinking_budget: -1        # Dynamic thinking budget
      gemini-3-flash:
        capabilities: [chat, thinking_traces]
        context_window: 2097152
        pricing:
          input_per_mtok: 200000     # $0.20 per million tokens
          output_per_mtok: 800000    # $0.80 per million tokens
        extra:
          thinking_level: "medium"
      gemini-3-pro:
        capabilities: [chat, thinking_traces]
        context_window: 2097152
        pricing:
          input_per_mtok: 2500000    # $2.50 per million tokens
          output_per_mtok: 15000000  # $15.00 per million tokens
        extra:
          thinking_level: "high"
      deep-research-pro:
        capabilities: [chat, thinking_traces, deep_research]
        context_window: 1048576
        api_mode: interactions
        pricing:
          input_per_mtok: 5000000    # $5.00 per million tokens
          output_per_mtok: 20000000  # $20.00 per million tokens
        extra:
          polling_interval_s: 5
          max_poll_time_s: 600

  anthropic:
    type: anthropic
    endpoint: "https://api.anthropic.com/v1"
    auth: "{env:ANTHROPIC_API_KEY}"
    models:
      claude-opus-4-6:
        capabilities: [chat, tools, function_calling, thinking_traces]
        context_window: 200000
        token_param: max_tokens
        pricing:
          input_per_mtok: 5000
          output_per_mtok: 25000
      claude-sonnet-4-6:
        capabilities: [chat, tools, function_calling]
        context_window: 200000
        token_param: max_tokens
        pricing:
          input_per_mtok: 3000
          output_per_mtok: 15000

# Aliases (short names → provider:model-id)
aliases:
  gemini-2.0: "google:gemini-2.0-flash"  # Backward-compat alias
  native: "claude-code:session"         # Reserved — Claude Code native runtime
  reviewer: "openai:gpt-5.3-codex"      # Primary review model
  reasoning: "openai:gpt-5.3-codex"    # Reasoning/skeptic model
  cheap: "anthropic:claude-sonnet-4-6"  # Budget-conscious model
  opus: "anthropic:claude-opus-4-6"     # High-quality model
  deep-thinker: "google:gemini-3-pro"   # Deep thinking with Gemini 3
  fast-thinker: "google:gemini-3-flash" # Fast thinking with Gemini 3
  researcher: "google:deep-research-pro" # Deep Research model

# Agent bindings (agent name → model + requirements)
# Conservative profile: maximize quality, most agents on native (PRD FR-4)
agents:
  implementing-tasks:
    model: native
    requires:
      native_runtime: true
  riding-codebase:
    model: native
    requires:
      native_runtime: true
  designing-architecture:
    model: native
  planning-sprints:
    model: native
  discovering-requirements:
    model: native
  reviewing-code:
    model: reviewer
    temperature: 0.3
  auditing-security:
    model: native
  translating-for-executives:
    model: cheap
    temperature: 0.5
  flatline-reviewer:
    model: reviewer
    temperature: 0.3
  flatline-skeptic:
    model: reasoning
    temperature: 0.5
    requires:
      thinking_traces: preferred
  flatline-scorer:
    model: reviewer
    temperature: 0.2
  flatline-dissenter:
    model: reasoning
    temperature: 0.6
    requires:
      thinking_traces: preferred
  gpt-reviewer:
    model: reviewer
    temperature: 0.3
  deep-researcher:
    model: researcher
    temperature: 0.3
    requires:
      deep_research: required
  deep-thinker:
    model: deep-thinker
    temperature: 0.4
    requires:
      thinking_traces: required
  fast-thinker:
    model: fast-thinker
    temperature: 0.5
  literature-reviewer:
    model: researcher
    temperature: 0.2
    requires:
      deep_research: required

  # Jam geometry roles (cycle-026 sprint-9)
  # Feature flag: hounfour.feature_flags.jam_geometry (default false)
  jam-reviewer-claude:
    model: native
    requires:
      native_runtime: true
      thinking_traces: true
  jam-reviewer-gpt:
    model: reviewer
    temperature: 0.3
  jam-reviewer-kimi:
    model: reasoning
    temperature: 0.5
    requires:
      thinking_traces: preferred
  jam-synthesizer:
    model: cheap
    temperature: 0.3

# Routing
routing:
  fallback:
    openai: [anthropic]           # If OpenAI down, try Anthropic
    anthropic: [openai]           # If Anthropic down, try OpenAI
    google: [openai]              # If Google down, try OpenAI
  downgrade:
    reviewer: [cheap]             # If budget exceeded, downgrade

  # Circuit breaker defaults
  circuit_breaker:
    failure_threshold: 5          # Consecutive failures to trip
    reset_timeout_seconds: 60     # Time in OPEN before probing
    half_open_max_probes: 1       # Concurrent probes in HALF_OPEN
    count_window_seconds: 300     # Rolling window for failure count

# Retry defaults
retry:
  max_retries: 3                  # Per-provider retries
  max_total_attempts: 6           # Global hard cap (SDD §4.2.7)
  max_provider_switches: 2        # Maximum fallback chain depth
  base_delay_seconds: 1.0         # Exponential backoff base

# Metering
metering:
  enabled: true
  ledger_path: "grimoires/loa/a2a/cost-ledger.jsonl"
  budget:
    daily_micro_usd: 500000000    # $500/day default (effectively unlimited)
    warn_at_percent: 80
    on_exceeded: downgrade        # downgrade | block | warn

# Default timeouts (per provider, overridable)
defaults:
  connect_timeout: 10             # seconds
  read_timeout: 120               # seconds
  write_timeout: 30               # seconds
